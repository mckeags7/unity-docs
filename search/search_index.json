{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Note The Unity docs are still a work-in-progress. Some docs may be missing, or incomplete. If you would like to contribute to these docs, please submit PRs to the upstream repository here . Introduction to Unity This introductory tutorial will help you build an understanding of what an HPC Cluster, or High Performance Computing Cluster is, and how to most effectively utilize it. Defining some Terms When working with HPCs, it can be helpful to understand what's happening behind the scenes. This tutorial will go over what an HPC is. A H igh P erformance C omputer-- HPC -- is super computer that is made up of many powerful computers that are called Clusters . A Cluster is made up of powerful computer parts that are connected to each other over a local network--this is akin to the motherboard on your local desktop/laptop. But for the purposes of this tutorial, it suffices to think of a Cluster as powerful computer, structurally similar to your personal desktop/laptop at home, just a lot more powerful. Think about your personal laptop/desktop. When you use your computer, the computer's Operating System (OS) decides what resources (cpu, ram, etc.) should be used, based on what you are doing at that time. For example, if you were to open the computer's calculator punch in some numbers and multiply them, the computer's OS would determine how much of the computer's CPU should be used to perform the calculation, and how much of the computer's graphics card to use in rendering the Graphical Interface of the calculator. In some cases, the rendering of the Graphical Interface and calculations could both be performed on the just the graphics card, or the CPU. Keeping with this idea of your OS dictating the distribution of computer resources, let's examine how a Cluser distributes its resources for an uploaded job ; a job is any operation or task you run on the cluster. When a job is loaded onto the Unity cluster, what is known as a scheduler determines what Cluster resources to give you, just like your personal computer's OS system would. You can picture the cluster as a scaled-up version of a single personal computer. Making up the cluster are Nodes . A Node is the part of the cluster that jobs get run on, like a GPU or CPU. When working with Clusters, it's easier to call these computer parts Nodes because sometimes just one of these computer parts is sufficient to run a job. In summary: High Performance Computers (HPC) : Super Computers made up of many Clusters that are interconnected over a local (within the same building) network. Cluster : Clusters are the computers that make up High Performace Computers (HPC) . They themselves are computers, and can be thought of like your desktop/laptop at home. Scheduler : Linux based distributer of resources. If you request 2 CPUs, and 2 GPUs, the Scheduler is what will make those resources available for you. This is akin to your computer's OS in that it assigns parts ( Nodes ) of the computer to you in order for those Nodes to run your software. The Scheduler for the Unity Cluster is called Slurm . Slurm sits on the Unity Cluster, and distributes the resources of the Unity Cluster accordingly. Nodes : Using the laptop/desktop comparison, a Node is akin to an individual part of the desktop/laptop, like the CPU or GPU where computations & programs are run. A Job : When something is uploaded onto the Cluster and used to run an operation, or when Cluster resources are utilized to run a program, this use of the Cluster's resources is called a Job . How Unity Works Here is a general step by step process that governs how the Unity Cluster works: The client connects to Unity using SSH , or the Jupyter Lab . (Bottom right of image) When you log into the Unity Cluster, you can interact with the Cluster either using Jupyter Lab, or Terminal. Note that there is a limited number of times that you can attempt to login. After GET NUMBER OF LOGIN FAILS failed login attempts. If you are not comfortable working with a Linux SSH terminal interface, it is recommended that you use Jupyter Lab. Once connected, a job is requested through the scheduler, and the scheduler adds your requests to each necessary que . ' Necessary ' because if the job requested requires 2 GPUs and 1 CPU then you need 2 positions in the GPU que and 1 position in the CPU que. This means that the wait time to get your required resources could be longer, but this doesn't mean the runtime of your job will also be longer. Once resources are available (cores, gpu, memory, etc.), the scheduler starts your job. Once your job completes, the result returns to the client. 1. Connecting to the cluster You can connect to Unity in two ways, an SSH connection (the standard linux console), or an instance of JupyterLab: JupyterLab is the easiest to get up and going. To work with JupyterLab, it's a good idea to get aquainted with roughly how demanding the job you're uploading is though. You just need to be familiar with how roughly how many of each resource you will need (Image below). Most of the time you will only ever need a single CPU or GPU, but if you have terabytes of data to analyze, then you should probably consider getting multiple GPUs and CPUs. When connecting the portal, click on JupyterLab tab located at the bottom of the options list on the left side of the window. This will take you to the JupyterHub for Unity, which looks like this: You will be asked to select what computer resources you want/need for the job you want to upload. Once you attempt to spawn your notebook and resources become available, you will be able to use JupyterLab as if it is running on your own computer. SSH is the more traditional method of using an HPC cluster. You will connect to the login node of unity, and you will be responsible for starting your own jobs. This can be more useful than JupyterLabs for jobs that last a long time and must be left unattended, or to have much more refined control over the resources allocated for your job. 2. Requesting Resources If you are on an SSH connection, you will have to manually request resources. Once you decide on what resources you want, you will submit that information to the scheduler, which will place you in a queue. If your resources are available immediately, your request will return right away, if not, you will be held in the queue until your requested resources become available. Requesting resources in the cluster and all parameters allowed is discussed in more detail here . 3. Starting Job Once the scheduler has started your job, it will run on some node in the cluster, using some resources that were defined by your parameters. It is not important what node the job runs on from the point of view of the client. 4. Ending Job Once the job has finished, the scheduler will return whatever info you requested in your parameters.","title":"Home"},{"location":"index.html#introduction-to-unity","text":"This introductory tutorial will help you build an understanding of what an HPC Cluster, or High Performance Computing Cluster is, and how to most effectively utilize it.","title":"Introduction to Unity"},{"location":"index.html#defining-some-terms","text":"When working with HPCs, it can be helpful to understand what's happening behind the scenes. This tutorial will go over what an HPC is. A H igh P erformance C omputer-- HPC -- is super computer that is made up of many powerful computers that are called Clusters . A Cluster is made up of powerful computer parts that are connected to each other over a local network--this is akin to the motherboard on your local desktop/laptop. But for the purposes of this tutorial, it suffices to think of a Cluster as powerful computer, structurally similar to your personal desktop/laptop at home, just a lot more powerful. Think about your personal laptop/desktop. When you use your computer, the computer's Operating System (OS) decides what resources (cpu, ram, etc.) should be used, based on what you are doing at that time. For example, if you were to open the computer's calculator punch in some numbers and multiply them, the computer's OS would determine how much of the computer's CPU should be used to perform the calculation, and how much of the computer's graphics card to use in rendering the Graphical Interface of the calculator. In some cases, the rendering of the Graphical Interface and calculations could both be performed on the just the graphics card, or the CPU. Keeping with this idea of your OS dictating the distribution of computer resources, let's examine how a Cluser distributes its resources for an uploaded job ; a job is any operation or task you run on the cluster. When a job is loaded onto the Unity cluster, what is known as a scheduler determines what Cluster resources to give you, just like your personal computer's OS system would. You can picture the cluster as a scaled-up version of a single personal computer. Making up the cluster are Nodes . A Node is the part of the cluster that jobs get run on, like a GPU or CPU. When working with Clusters, it's easier to call these computer parts Nodes because sometimes just one of these computer parts is sufficient to run a job.","title":"Defining some Terms"},{"location":"index.html#how-unity-works","text":"Here is a general step by step process that governs how the Unity Cluster works: The client connects to Unity using SSH , or the Jupyter Lab . (Bottom right of image) When you log into the Unity Cluster, you can interact with the Cluster either using Jupyter Lab, or Terminal. Note that there is a limited number of times that you can attempt to login. After GET NUMBER OF LOGIN FAILS failed login attempts. If you are not comfortable working with a Linux SSH terminal interface, it is recommended that you use Jupyter Lab. Once connected, a job is requested through the scheduler, and the scheduler adds your requests to each necessary que . ' Necessary ' because if the job requested requires 2 GPUs and 1 CPU then you need 2 positions in the GPU que and 1 position in the CPU que. This means that the wait time to get your required resources could be longer, but this doesn't mean the runtime of your job will also be longer. Once resources are available (cores, gpu, memory, etc.), the scheduler starts your job. Once your job completes, the result returns to the client.","title":"How Unity Works"},{"location":"index.html#1-connecting-to-the-cluster","text":"You can connect to Unity in two ways, an SSH connection (the standard linux console), or an instance of JupyterLab: JupyterLab is the easiest to get up and going. To work with JupyterLab, it's a good idea to get aquainted with roughly how demanding the job you're uploading is though. You just need to be familiar with how roughly how many of each resource you will need (Image below). Most of the time you will only ever need a single CPU or GPU, but if you have terabytes of data to analyze, then you should probably consider getting multiple GPUs and CPUs. When connecting the portal, click on JupyterLab tab located at the bottom of the options list on the left side of the window. This will take you to the JupyterHub for Unity, which looks like this: You will be asked to select what computer resources you want/need for the job you want to upload. Once you attempt to spawn your notebook and resources become available, you will be able to use JupyterLab as if it is running on your own computer. SSH is the more traditional method of using an HPC cluster. You will connect to the login node of unity, and you will be responsible for starting your own jobs. This can be more useful than JupyterLabs for jobs that last a long time and must be left unattended, or to have much more refined control over the resources allocated for your job.","title":"1. Connecting to the cluster"},{"location":"index.html#2-requesting-resources","text":"If you are on an SSH connection, you will have to manually request resources. Once you decide on what resources you want, you will submit that information to the scheduler, which will place you in a queue. If your resources are available immediately, your request will return right away, if not, you will be held in the queue until your requested resources become available. Requesting resources in the cluster and all parameters allowed is discussed in more detail here .","title":"2. Requesting Resources"},{"location":"index.html#3-starting-job","text":"Once the scheduler has started your job, it will run on some node in the cluster, using some resources that were defined by your parameters. It is not important what node the job runs on from the point of view of the client.","title":"3. Starting Job"},{"location":"index.html#4-ending-job","text":"Once the job has finished, the scheduler will return whatever info you requested in your parameters.","title":"4. Ending Job"},{"location":"faq.html","text":"Note The Unity docs are still a work-in-progress. Some docs may be missing, or incomplete. If you would like to contribute to these docs, please submit PRs to the upstream repository here . Frequently Asked Questions How do I connect to, and start using the cluster? Refer to connection instructions on connecting here . You can connect with Putty, SSH in your terminal, or JupyterLab in your browser. When I connect over SSH I get a message saying \"permission denied (public key)\" This can be due one of these common reasons: You have not provided your private key while connecting. ssh -i <private_key_location> <user>@unity.rc.umass.edu You are not assigned to at least one PI group. We require at least one PI to endorse your account before you can use the cluster. Request to join a PI on the My PIs page. You have not added a public key to your account on Unity yet. You can do this on the Account Settings page. Your login shell is invalid. In Account Settings, try \"/bin/bash\" or \"/bin/zsh\". You are a PI, and you are trying to use your PI group name to log in. Your login username should not start with 'pi_'. Where can I find software to use on the cluster? Most of our software is package installed and is available by default. Non standard and version specific software are available as modules. The command module av will print all available modules. module av <name> will filter the available modules. Then you can use module load <name> to load a module and have access to its binaries (executables). I'm looking for xyz software, could you install it? Most software that is requested is free for use. If this is the case we will install it for you, just send us an email at hpc@umass.edu titled \"software request: \\<name>\". If the software you want is licensed, we may be able to help since the campus often has site-wide licenses for many applications. Can I run containers on Unity? Yes! We support singularity containers, which are fully compatible with docker images. Run \"module load singularity\" to access it. How much storage do I get on Unity and is it backed up? Refer to storage information here . We do not provide backup solutions by default. We take a snapshot of all storage at 1 AM every day for the past 48 hours. This way, if you accidentally deleted something it wouldn't be difficult to get it back within that time frame. When I try to queue a job I get denied for MaxCpuPerAccount. Resource limits are set per lab. Currently, they are 300 CPUs, and 64 GPUs. This allocation is shared across your entire PI group. I'm a PI and I would like to purchase hardware to buy-in to Unity. Great! Send us an email and we'll be happy to help. We are very flexible when it comes to the needs of research labs.","title":"FAQ"},{"location":"faq.html#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"faq.html#how-do-i-connect-to-and-start-using-the-cluster","text":"Refer to connection instructions on connecting here . You can connect with Putty, SSH in your terminal, or JupyterLab in your browser.","title":"How do I connect to, and start using the cluster?"},{"location":"faq.html#when-i-connect-over-ssh-i-get-a-message-saying-permission-denied-public-key","text":"This can be due one of these common reasons: You have not provided your private key while connecting. ssh -i <private_key_location> <user>@unity.rc.umass.edu You are not assigned to at least one PI group. We require at least one PI to endorse your account before you can use the cluster. Request to join a PI on the My PIs page. You have not added a public key to your account on Unity yet. You can do this on the Account Settings page. Your login shell is invalid. In Account Settings, try \"/bin/bash\" or \"/bin/zsh\". You are a PI, and you are trying to use your PI group name to log in. Your login username should not start with 'pi_'.","title":"When I connect over SSH I get a message saying \"permission denied (public key)\""},{"location":"faq.html#where-can-i-find-software-to-use-on-the-cluster","text":"Most of our software is package installed and is available by default. Non standard and version specific software are available as modules. The command module av will print all available modules. module av <name> will filter the available modules. Then you can use module load <name> to load a module and have access to its binaries (executables).","title":"Where can I find software to use on the cluster?"},{"location":"faq.html#im-looking-for-xyz-software-could-you-install-it","text":"Most software that is requested is free for use. If this is the case we will install it for you, just send us an email at hpc@umass.edu titled \"software request: \\<name>\". If the software you want is licensed, we may be able to help since the campus often has site-wide licenses for many applications.","title":"I'm looking for xyz software, could you install it?"},{"location":"faq.html#can-i-run-containers-on-unity","text":"Yes! We support singularity containers, which are fully compatible with docker images. Run \"module load singularity\" to access it.","title":"Can I run containers on Unity?"},{"location":"faq.html#how-much-storage-do-i-get-on-unity-and-is-it-backed-up","text":"Refer to storage information here . We do not provide backup solutions by default. We take a snapshot of all storage at 1 AM every day for the past 48 hours. This way, if you accidentally deleted something it wouldn't be difficult to get it back within that time frame.","title":"How much storage do I get on Unity and is it backed up?"},{"location":"faq.html#when-i-try-to-queue-a-job-i-get-denied-for-maxcpuperaccount","text":"Resource limits are set per lab. Currently, they are 300 CPUs, and 64 GPUs. This allocation is shared across your entire PI group.","title":"When I try to queue a job I get denied for MaxCpuPerAccount."},{"location":"faq.html#im-a-pi-and-i-would-like-to-purchase-hardware-to-buy-in-to-unity","text":"Great! Send us an email and we'll be happy to help. We are very flexible when it comes to the needs of research labs.","title":"I'm a PI and I would like to purchase hardware to buy-in to Unity."},{"location":"mpi.html","text":"Running Jobs with Shared Memory Using MPI This is the (simplified) sanity check that the Unity admins used to verify openmpi: Note We recommend when running quick jobs that you put your job in a preempt partition for a shorter wait time. srun -p cpu-preempt ... srun module load openmpi mpicc /modules/admin-resources/mpi_testing/mpi_array.c -o mpi_array srun --pty -N 2 mpi_array sbatch module load openmpi mpicc /modules/admin-resources/mpi_testing/mpi_array.c -o mpi_array sbatch -N 2 mpi_script Where mpi_script is a file containing the following: #!/bin/bash srun mpi_array Frequently Asked Questions Why can't I use mpirun / mpiexec ? This version of Spack (openmpi ~legacylaunchers schedulers=slurm) is installed without the mpiexec/mpirun commands to prevent unintended performance issues. See https://github.com/spack/spack/pull/10340 for more details. If you understand the potential consequences of a misconfigured mpirun, you can use spack to install 'openmpi+legacylaunchers' to restore the executables. Otherwise, use srun to launch your MPI executables. The community of HPC admins at Spack have agreed that using mpirun with slurm is a bad idea. srun is capable of doing all that mpirun is, and having the two fight over control is reported to cause poor performance. We currently have a version of openmpi called openmpi+mpirun , which was installed in Spack with +legacylaunchers , enabling you to use the wrappers as you please. Why do I get multiple outputs from my mpi aware binaries? This is because memory sharing is not working. You are running duplicates of your job in parallel, which are unaware of each other. If using srun , you should make sure that you have the pseudo-terminal enabled --pty . This should not occur with sbatch . Why do I keep getting PMIX ERROR: NO-PERMISSIONS in file ? openmpi is dependent on pmix. Our current system slurm installation was not configured with pmix support. This is evident by srun --mpi=list . pmix_v2 and pmix_v3 should be there, but they aren't. We will likely recomile slurm to accomodate this. Why do I keep getting this OpenFabrics warning? \"No OpenFabrics connection schemes reported that they were able to be used on a specific port. As such, the openib BTL (OpenFabrics support) will be disabled for this port.\" We do not currently have infiniband hardware in our network, and openmpi would like us to. You can simply add -mca btl ^ofi to your mpirun command and disable the infiniband feature. We will likely recompile openmpi to disable this sitewide. mpirun -mca btl ^ofi ...","title":"MPI"},{"location":"mpi.html#running-jobs-with-shared-memory-using-mpi","text":"","title":"Running Jobs with Shared Memory Using MPI"},{"location":"mpi.html#this-is-the-simplified-sanity-check-that-the-unity-admins-used-to-verify-openmpi","text":"Note We recommend when running quick jobs that you put your job in a preempt partition for a shorter wait time. srun -p cpu-preempt ...","title":"This is the (simplified) sanity check that the Unity admins used to verify openmpi:"},{"location":"mpi.html#srun","text":"module load openmpi mpicc /modules/admin-resources/mpi_testing/mpi_array.c -o mpi_array srun --pty -N 2 mpi_array","title":"srun"},{"location":"mpi.html#sbatch","text":"module load openmpi mpicc /modules/admin-resources/mpi_testing/mpi_array.c -o mpi_array sbatch -N 2 mpi_script Where mpi_script is a file containing the following: #!/bin/bash srun mpi_array","title":"sbatch"},{"location":"mpi.html#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"mpi.html#why-cant-i-use-mpirunmpiexec","text":"This version of Spack (openmpi ~legacylaunchers schedulers=slurm) is installed without the mpiexec/mpirun commands to prevent unintended performance issues. See https://github.com/spack/spack/pull/10340 for more details. If you understand the potential consequences of a misconfigured mpirun, you can use spack to install 'openmpi+legacylaunchers' to restore the executables. Otherwise, use srun to launch your MPI executables. The community of HPC admins at Spack have agreed that using mpirun with slurm is a bad idea. srun is capable of doing all that mpirun is, and having the two fight over control is reported to cause poor performance. We currently have a version of openmpi called openmpi+mpirun , which was installed in Spack with +legacylaunchers , enabling you to use the wrappers as you please.","title":"Why can't I use mpirun/mpiexec?"},{"location":"mpi.html#why-do-i-get-multiple-outputs-from-my-mpi-aware-binaries","text":"This is because memory sharing is not working. You are running duplicates of your job in parallel, which are unaware of each other. If using srun , you should make sure that you have the pseudo-terminal enabled --pty . This should not occur with sbatch .","title":"Why do I get multiple outputs from my mpi aware binaries?"},{"location":"mpi.html#why-do-i-keep-getting-pmix-error-no-permissions-in-file","text":"openmpi is dependent on pmix. Our current system slurm installation was not configured with pmix support. This is evident by srun --mpi=list . pmix_v2 and pmix_v3 should be there, but they aren't. We will likely recomile slurm to accomodate this.","title":"Why do I keep getting PMIX ERROR: NO-PERMISSIONS in file?"},{"location":"mpi.html#why-do-i-keep-getting-this-openfabrics-warning","text":"\"No OpenFabrics connection schemes reported that they were able to be used on a specific port. As such, the openib BTL (OpenFabrics support) will be disabled for this port.\" We do not currently have infiniband hardware in our network, and openmpi would like us to. You can simply add -mca btl ^ofi to your mpirun command and disable the infiniband feature. We will likely recompile openmpi to disable this sitewide. mpirun -mca btl ^ofi ...","title":"Why do I keep getting this OpenFabrics warning?"},{"location":"roadmap.html","text":"Unity Cluster Roadmap Unity is in Beta! With the addition of additional service hardware and storage devices, we are more confident in the stability of the cluster, and thus are ready to call it in \"beta\" state. In beta phase expect planned downtime more frequently than production research clusters, but you can expect the downtime periods to be shorter and more informative. General Wider support for common applications Support for singularity containers within Slurm VNC sessions from within jupyterhub for running GUI applications Infiniband support for inter-node MPI Graphical usage report Support for multiple PIs attached to one user Support for dynamic creation of shared drives Website Finish writing user documenation. Add content to the landing page(s) Hardware Multiple service nodes for reliability High-speed flash storage node for scratch space 600+ TB spinning disk NAS for general storage 32 GPU nodes with 8 2080TIs each inside","title":"Unity Cluster Roadmap #"},{"location":"roadmap.html#unity-cluster-roadmap","text":"Unity is in Beta! With the addition of additional service hardware and storage devices, we are more confident in the stability of the cluster, and thus are ready to call it in \"beta\" state. In beta phase expect planned downtime more frequently than production research clusters, but you can expect the downtime periods to be shorter and more informative.","title":"Unity Cluster Roadmap"},{"location":"roadmap.html#general","text":"Wider support for common applications Support for singularity containers within Slurm VNC sessions from within jupyterhub for running GUI applications Infiniband support for inter-node MPI Graphical usage report Support for multiple PIs attached to one user Support for dynamic creation of shared drives","title":"General"},{"location":"roadmap.html#website","text":"Finish writing user documenation. Add content to the landing page(s)","title":"Website"},{"location":"roadmap.html#hardware","text":"Multiple service nodes for reliability High-speed flash storage node for scratch space 600+ TB spinning disk NAS for general storage 32 GPU nodes with 8 2080TIs each inside","title":"Hardware"},{"location":"buy-in/requirements.html","text":"Hardware Requirements Note This page is here for reference. Please do not purchase hardware intended for the Unity cluster until checking with the Unity team. General Node Requirements Each server purchased is required to have the following hard requirements: IPMI 2.0 supported BMC with remote KVM built-in with discrete RJ45 management port Physical Ports VGA Port USB Port 10G SFP+ Port or 25G SFP28 port Note: If you require higher bandwidth for your nodes, that can be discussed, but will most likely require purchasing an addition switch (see below) At least 250GB boot SSD (Solid-state drive) 2x redundant PSU (Power supply unit) Each server purchased is recommended to have the following soft requirements: 2x boot drive for redundancy Storage Node Requirements The storage controller should be an HBA card or a RAID card that can operate in HBA mode, since we will be using ZFS software raid. At least 2 data plane (10/25G) ports for redundant connection. 2x boot drives for redundancy Network Equipment On a case-by-case basis, it may be necessary to purchase network switches if enough hardware was purchased to span a rack. If that is the case, each rack requires the following for the top-of-rack data switch: For 10/25G Nodes Mellanox SN2410 For 100G Nodes Mellanox SN2700 In addition, a 1G switch is required for IPMI connection. For IPMI Any 1G switch that has L2 capability (Mellanox preferred)","title":"Hardware Requirements"},{"location":"buy-in/requirements.html#hardware-requirements","text":"Note This page is here for reference. Please do not purchase hardware intended for the Unity cluster until checking with the Unity team.","title":"Hardware Requirements"},{"location":"buy-in/requirements.html#general-node-requirements","text":"Each server purchased is required to have the following hard requirements: IPMI 2.0 supported BMC with remote KVM built-in with discrete RJ45 management port Physical Ports VGA Port USB Port 10G SFP+ Port or 25G SFP28 port Note: If you require higher bandwidth for your nodes, that can be discussed, but will most likely require purchasing an addition switch (see below) At least 250GB boot SSD (Solid-state drive) 2x redundant PSU (Power supply unit) Each server purchased is recommended to have the following soft requirements: 2x boot drive for redundancy","title":"General Node Requirements"},{"location":"buy-in/requirements.html#storage-node-requirements","text":"The storage controller should be an HBA card or a RAID card that can operate in HBA mode, since we will be using ZFS software raid. At least 2 data plane (10/25G) ports for redundant connection. 2x boot drives for redundancy","title":"Storage Node Requirements"},{"location":"buy-in/requirements.html#network-equipment","text":"On a case-by-case basis, it may be necessary to purchase network switches if enough hardware was purchased to span a rack. If that is the case, each rack requires the following for the top-of-rack data switch: For 10/25G Nodes Mellanox SN2410 For 100G Nodes Mellanox SN2700 In addition, a 1G switch is required for IPMI connection. For IPMI Any 1G switch that has L2 capability (Mellanox preferred)","title":"Network Equipment"},{"location":"buy-in/types.html","text":"Types of Priority Nodes you purchase will be included in the Unity ecosystem - meaning you'll be able to use the web portal, access all the same storage, and access JupyterLab. When purchasing hardware for integration with Unity, there are three types of \"priority\" that you can request for your hardware. Note The below only applies to compute hardware, storage hardware will almost always be owned by the purchasing PI only and not shared with users. Preemption (Strictest) Priority nodes are added to: preempt partitions (cpu-preempt/gpu-preempt), and a newly created lab partition for use only by your lab Preemption is enabled in this mode. This means that if a priority user (member of the lab) wants to start a job but the nodes are full, non-priority jobs will be requeued on the spot for priority jobs. The requeued job will start again once resources are available. Your nodes are added to cpu/gpu-preempt partition depending on the type, in addition to the priority partition. This results in immediate access to the full capabilities of your own hardware, but the strictest limitations for general users of the cluster. Queueing Priority Priority nodes are added to: general partitions (cpu/gpu), and a newly created lab partition for use only by your lab Preemption is disabled in this mode, but your lab still gets queueing priority, which means if jobs are waiting to start on your hardware, your lab's jobs will always start first and take precedent. However, if your nodes are already full of general jobs, you will not be able to access this space until those jobs are done. As a result, we generally restrict general jobs to short queue timing (24 hours) such that priority users will not need to wait over a week etc. for non-priority jobs. No Priority In this mode no priority partition is created, and your nodes are contributed directly to the general partitions. It is not clear how this will affect billing for general hardware yet.","title":"Types of Priority"},{"location":"buy-in/types.html#types-of-priority","text":"Nodes you purchase will be included in the Unity ecosystem - meaning you'll be able to use the web portal, access all the same storage, and access JupyterLab. When purchasing hardware for integration with Unity, there are three types of \"priority\" that you can request for your hardware. Note The below only applies to compute hardware, storage hardware will almost always be owned by the purchasing PI only and not shared with users.","title":"Types of Priority"},{"location":"buy-in/types.html#preemption-strictest","text":"Priority nodes are added to: preempt partitions (cpu-preempt/gpu-preempt), and a newly created lab partition for use only by your lab Preemption is enabled in this mode. This means that if a priority user (member of the lab) wants to start a job but the nodes are full, non-priority jobs will be requeued on the spot for priority jobs. The requeued job will start again once resources are available. Your nodes are added to cpu/gpu-preempt partition depending on the type, in addition to the priority partition. This results in immediate access to the full capabilities of your own hardware, but the strictest limitations for general users of the cluster.","title":"Preemption (Strictest)"},{"location":"buy-in/types.html#queueing-priority","text":"Priority nodes are added to: general partitions (cpu/gpu), and a newly created lab partition for use only by your lab Preemption is disabled in this mode, but your lab still gets queueing priority, which means if jobs are waiting to start on your hardware, your lab's jobs will always start first and take precedent. However, if your nodes are already full of general jobs, you will not be able to access this space until those jobs are done. As a result, we generally restrict general jobs to short queue timing (24 hours) such that priority users will not need to wait over a week etc. for non-priority jobs.","title":"Queueing Priority"},{"location":"buy-in/types.html#no-priority","text":"In this mode no priority partition is created, and your nodes are contributed directly to the general partitions. It is not clear how this will affect billing for general hardware yet.","title":"No Priority"},{"location":"connecting/jupyter.html","text":"JupyterHub Portal The simplest method of using the cluster is using the Jupyter portal. You can access the jupyter portal from the Home Page > \"JupyterLab\". Simply select the resources you want and get started right away. Comprehensive documentation for JupyterLab can be found here .","title":"Web - JupyterLab"},{"location":"connecting/jupyter.html#jupyterhub-portal","text":"The simplest method of using the cluster is using the Jupyter portal. You can access the jupyter portal from the Home Page > \"JupyterLab\". Simply select the resources you want and get started right away. Comprehensive documentation for JupyterLab can be found here .","title":"JupyterHub Portal"},{"location":"connecting/ssh.html","text":"SSH Connection The most traditional method of connecting to Unity is using an SSH connection. A shell is what you type commands into. The most common shell in linux is bash, which is what you will likely be using on Unity. SSH stands for \"secure shell\". Configure SSH Keys The authentication method we use for SSH connections is with public/private RSA keys. You can read more about the public/private key exchange here . For the purposes of this guide, you should know that there is a public key which is stored on the server, and a private key, which you keep on your local computer. Think of them like your name and your social security number, respectively. In very basic terms, you authenticate the public key with your private key and that allows you to login to Unity. You must save your public key on Unity by adding it in your account settings . If you are unsure how to generate a public/private key pair, simply click on 'Generate Key'. The public key will be added to our database, and the private key will be downloaded to your computer. Note It's recommended that you place this downloaded private key in your home directory's .ssh folder. This is C:/Users/YOUR_NAME/.ssh in Windows, /home/YOUR_NAME/.ssh in Linux, and /Users/YOUR_NAME in Mac . In the terminal, a shortcut for this directory is the ~ symbol. This command will make the move on any operating system: mv ~/Downloads/privkey.key ~/.ssh/unity-privkey.key On Linux/Mac, you will need to change the permissions on the file due to its importance to security. chmod 600 ~/.ssh/unity-privkey.key It's recommended that you also add a password to this file using the following command: ssh-keygen -p -f ~/.ssh/unity-privkey.rsa Connection Details If you know what to do with this information already, you can skip the rest of this guide. Hostname/Address: unity.rc.umass.edu Username: NETID_school_edu Note Your username should be your email with all at's @ and dot's . replaced with underscores _ . View your username here Windows GUI Users Windows users can use PuTTY to connect to Unity. Download and install PuTTY by following the link above. Be sure to select the 64 bit / 32 bit download depending on your system. Most are 64 bit, but if you are unsure 32 bit will always work. Open PuTTY and enter hostname unity.rc.umass.edu on the main page On the left sidebar, navigate to connection -> data, and enter your username. On the left sidebar, navigate to connction -> ssh -> auth, and browse to your private key location. Finally, in the main screen again, save the profile you created so you don't have to enter this information every time. Enter unity as the profile name and click save . You can then double click on unity under saved sessions to connect to Unity right away. CLI Users Windows, Mac, and most distributions of linux come with the OpenSSH client, which you can use to connect to Unity in your terminal. If the file ~/.ssh/config doesn't exist, create it. Append the following contents and replace <NETID> and <PATH_TO_PRIVATE_KEY> to your specifications: Host unity HostName unity.rc.umass.edu User <NETID>_umass_edu IdentityFile <PATH_TO_PRIVATE_KEY> Note Doing this with a text editor and a file explorer can be challenging because these user friendly methods don't like files without an extension, and the ssh config file must not have an extension. In Windows Notepad, you can save a file with no extension in the 'All Files' category, and windows will add the .txt extension regardless, which won't work. The Mac TextEdit doesn't even have the option to save as .txt , which is tremendously unhelpful. You can make your current file plain-text formatted using \u2318-\u21e7-T, and you can add plain-text as a 'Save as' option in the config. The most reliable way to put your OpenSSH config file in the correct location is to open the terminal and use the mv (move) command, which will rename files with no fuss. mv path/to/source-file path/to/desination-file mv ~/Desktop/ssh-config.txt ~/.ssh/config Once the OpenSSH config file is in place, you can connect to Unity in your terminal using the command ssh unity .","title":"Console - SSH"},{"location":"connecting/ssh.html#ssh-connection","text":"The most traditional method of connecting to Unity is using an SSH connection. A shell is what you type commands into. The most common shell in linux is bash, which is what you will likely be using on Unity. SSH stands for \"secure shell\".","title":"SSH Connection"},{"location":"connecting/ssh.html#configure-ssh-keys","text":"The authentication method we use for SSH connections is with public/private RSA keys. You can read more about the public/private key exchange here . For the purposes of this guide, you should know that there is a public key which is stored on the server, and a private key, which you keep on your local computer. Think of them like your name and your social security number, respectively. In very basic terms, you authenticate the public key with your private key and that allows you to login to Unity. You must save your public key on Unity by adding it in your account settings . If you are unsure how to generate a public/private key pair, simply click on 'Generate Key'. The public key will be added to our database, and the private key will be downloaded to your computer. Note It's recommended that you place this downloaded private key in your home directory's .ssh folder. This is C:/Users/YOUR_NAME/.ssh in Windows, /home/YOUR_NAME/.ssh in Linux, and /Users/YOUR_NAME in Mac . In the terminal, a shortcut for this directory is the ~ symbol. This command will make the move on any operating system: mv ~/Downloads/privkey.key ~/.ssh/unity-privkey.key On Linux/Mac, you will need to change the permissions on the file due to its importance to security. chmod 600 ~/.ssh/unity-privkey.key It's recommended that you also add a password to this file using the following command: ssh-keygen -p -f ~/.ssh/unity-privkey.rsa","title":"Configure SSH Keys"},{"location":"connecting/ssh.html#connection-details","text":"If you know what to do with this information already, you can skip the rest of this guide. Hostname/Address: unity.rc.umass.edu Username: NETID_school_edu Note Your username should be your email with all at's @ and dot's . replaced with underscores _ . View your username here","title":"Connection Details"},{"location":"connecting/ssh.html#windows-gui-users","text":"Windows users can use PuTTY to connect to Unity. Download and install PuTTY by following the link above. Be sure to select the 64 bit / 32 bit download depending on your system. Most are 64 bit, but if you are unsure 32 bit will always work. Open PuTTY and enter hostname unity.rc.umass.edu on the main page On the left sidebar, navigate to connection -> data, and enter your username. On the left sidebar, navigate to connction -> ssh -> auth, and browse to your private key location. Finally, in the main screen again, save the profile you created so you don't have to enter this information every time. Enter unity as the profile name and click save . You can then double click on unity under saved sessions to connect to Unity right away.","title":"Windows GUI Users"},{"location":"connecting/ssh.html#cli-users","text":"Windows, Mac, and most distributions of linux come with the OpenSSH client, which you can use to connect to Unity in your terminal. If the file ~/.ssh/config doesn't exist, create it. Append the following contents and replace <NETID> and <PATH_TO_PRIVATE_KEY> to your specifications: Host unity HostName unity.rc.umass.edu User <NETID>_umass_edu IdentityFile <PATH_TO_PRIVATE_KEY> Note Doing this with a text editor and a file explorer can be challenging because these user friendly methods don't like files without an extension, and the ssh config file must not have an extension. In Windows Notepad, you can save a file with no extension in the 'All Files' category, and windows will add the .txt extension regardless, which won't work. The Mac TextEdit doesn't even have the option to save as .txt , which is tremendously unhelpful. You can make your current file plain-text formatted using \u2318-\u21e7-T, and you can add plain-text as a 'Save as' option in the config. The most reliable way to put your OpenSSH config file in the correct location is to open the terminal and use the mv (move) command, which will rename files with no fuss. mv path/to/source-file path/to/desination-file mv ~/Desktop/ssh-config.txt ~/.ssh/config Once the OpenSSH config file is in place, you can connect to Unity in your terminal using the command ssh unity .","title":"CLI Users"},{"location":"jupyter/conda.html","text":"Conda Environments within JupyterLab Note Currently, there is no GUI implementation within JupyterLab of the below processes. This is a planned feature for the future. Introduction It is possible to create unlimited custom conda environments and use them within JupyterLab. Some CLI commands are required, but they can all be done through the JupyterLab interface. The Conda package manager does not require any compiling during package install, which is why it is favorable in a cluster environment where users don't have admin right to install dependent libraries. Environment Setup Note This guide uses the conda command often. Thie command needs to be loaded before using it using the command module load miniconda . This guide assumes you have done this. Creating an Environment Create an environment using this command. You can create as many discrete environment as you want. conda create --name testName python=3.7 Note Replace testName with the name of your choice for the environment, and replace 3.7 with whatever python version you want. Activating an Environment Use this command to activate your environment. This step is required any time you'd like to modify that specific environment. conda activate testName Note Your currently active conda environment will appear left of your prompt in the format (testName) . Adding Packages to your Environment Install Conda packages of your choosing. We are using numpy as an example. conda install numpy The install will ask you to confirm installing numpy as well as any other additional required packages, you will need to repond with a y then press enter. Adding your Environment to JupyterLab Make sure your environment is active. The ipykernel package has to be installed within your environment for JupyterLab to be able to talk to it. Other kernels are supported, since conda can run languages other than python. conda install ipykernel Add a kernelspec (Kernel Specification) to your JupyterLab. python -m ipykernel install --user --name testName --display-name=\"Display Name Within JupyterLab\" If the above was done within JupyterLab, your JupyterLab instance must be restarted. Other Operations List Available Environments conda env list List Packages Currently Installed in the Activated Environment conda list Deleteing an Environment conda remove --name testName --all You may also need to remove the kernelspec within JupyterLab seperately. rm -rf ~/.local/share/jupyter/kernels/testName","title":"Conda Environments within JupyterLab #"},{"location":"jupyter/conda.html#conda-environments-within-jupyterlab","text":"Note Currently, there is no GUI implementation within JupyterLab of the below processes. This is a planned feature for the future.","title":"Conda Environments within JupyterLab"},{"location":"jupyter/conda.html#introduction","text":"It is possible to create unlimited custom conda environments and use them within JupyterLab. Some CLI commands are required, but they can all be done through the JupyterLab interface. The Conda package manager does not require any compiling during package install, which is why it is favorable in a cluster environment where users don't have admin right to install dependent libraries.","title":"Introduction"},{"location":"jupyter/conda.html#environment-setup","text":"Note This guide uses the conda command often. Thie command needs to be loaded before using it using the command module load miniconda . This guide assumes you have done this.","title":"Environment Setup"},{"location":"jupyter/conda.html#creating-an-environment","text":"Create an environment using this command. You can create as many discrete environment as you want. conda create --name testName python=3.7 Note Replace testName with the name of your choice for the environment, and replace 3.7 with whatever python version you want.","title":"Creating an Environment"},{"location":"jupyter/conda.html#activating-an-environment","text":"Use this command to activate your environment. This step is required any time you'd like to modify that specific environment. conda activate testName Note Your currently active conda environment will appear left of your prompt in the format (testName) .","title":"Activating an Environment"},{"location":"jupyter/conda.html#adding-packages-to-your-environment","text":"Install Conda packages of your choosing. We are using numpy as an example. conda install numpy The install will ask you to confirm installing numpy as well as any other additional required packages, you will need to repond with a y then press enter.","title":"Adding Packages to your Environment"},{"location":"jupyter/conda.html#adding-your-environment-to-jupyterlab","text":"Make sure your environment is active. The ipykernel package has to be installed within your environment for JupyterLab to be able to talk to it. Other kernels are supported, since conda can run languages other than python. conda install ipykernel Add a kernelspec (Kernel Specification) to your JupyterLab. python -m ipykernel install --user --name testName --display-name=\"Display Name Within JupyterLab\" If the above was done within JupyterLab, your JupyterLab instance must be restarted.","title":"Adding your Environment to JupyterLab"},{"location":"jupyter/conda.html#other-operations","text":"","title":"Other Operations"},{"location":"jupyter/conda.html#list-available-environments","text":"conda env list","title":"List Available Environments"},{"location":"jupyter/conda.html#list-packages-currently-installed-in-the-activated-environment","text":"conda list","title":"List Packages Currently Installed in the Activated Environment"},{"location":"jupyter/conda.html#deleteing-an-environment","text":"conda remove --name testName --all You may also need to remove the kernelspec within JupyterLab seperately. rm -rf ~/.local/share/jupyter/kernels/testName","title":"Deleteing an Environment"},{"location":"modules/index.html","text":"Environment Modules Environment modules is how we are able to provide one installation of a piece of software to an entire cluster and its user base. Modules also allow us to have infinite versions of a single piece of software without conflicts. A module is a piece of software that the user can load in his/her shell, and then have access to that software. The software is installed elsewhere, but is not available until the user loads it. Unity uses lmod for managing modules. How it Works You can skip this part if you don't care about how it works, but it may allow you to gain a better understanding for future reference. Environment modules work by modifying the user's environment. The user's environment has a number of variables set. To see them all, just run env in the cli if you are curious. Those variables dictate what the user has access to. For instance, consider the $PATH environment variable. This variable is what determines where to find executable binaries. For example, when you run the command ls to list the directory, ls is simply an executable that lies somewhere on the system. In this case, /bin . Ordinarily, to run the ls executable, you would need to reference it directly, ie /bin/ls . To simplify this, linux adds certain directories to the $PATH environment variable. Any binaries/executable contained within any directories defined in $PATH can be executed from anywhere directly, ie ls . As such, /bin , /usr/bin , /usr/local/bin , etc. are all members of the $PATH variable. Modulefiles work by manipulating these variables. For example, consider python , a binary that comes with almost all linux distributions these days. Usually, running python will execute the python that is installed on the system. But if we wanted to create a module which loads a different python, we would add the location of the different python into the environment variables, at the top of the list. Now, when you run python , the first entry in $PATH directs you to the module you loaded, and it loads that, even though a system python exists later on. The important thing is that the program you want to use is first in the variable. To see physical examples of modulefiles, please visit this page .","title":"Introduction"},{"location":"modules/index.html#environment-modules","text":"Environment modules is how we are able to provide one installation of a piece of software to an entire cluster and its user base. Modules also allow us to have infinite versions of a single piece of software without conflicts. A module is a piece of software that the user can load in his/her shell, and then have access to that software. The software is installed elsewhere, but is not available until the user loads it. Unity uses lmod for managing modules.","title":"Environment Modules"},{"location":"modules/index.html#how-it-works","text":"You can skip this part if you don't care about how it works, but it may allow you to gain a better understanding for future reference. Environment modules work by modifying the user's environment. The user's environment has a number of variables set. To see them all, just run env in the cli if you are curious. Those variables dictate what the user has access to. For instance, consider the $PATH environment variable. This variable is what determines where to find executable binaries. For example, when you run the command ls to list the directory, ls is simply an executable that lies somewhere on the system. In this case, /bin . Ordinarily, to run the ls executable, you would need to reference it directly, ie /bin/ls . To simplify this, linux adds certain directories to the $PATH environment variable. Any binaries/executable contained within any directories defined in $PATH can be executed from anywhere directly, ie ls . As such, /bin , /usr/bin , /usr/local/bin , etc. are all members of the $PATH variable. Modulefiles work by manipulating these variables. For example, consider python , a binary that comes with almost all linux distributions these days. Usually, running python will execute the python that is installed on the system. But if we wanted to create a module which loads a different python, we would add the location of the different python into the environment variables, at the top of the list. Now, when you run python , the first entry in $PATH directs you to the module you loaded, and it loads that, even though a system python exists later on. The important thing is that the program you want to use is first in the variable. To see physical examples of modulefiles, please visit this page .","title":"How it Works"},{"location":"modules/using.html","text":"Using Modulefiles Using modulefiles is very simple. Keep in mind that the below commands will work anywhere on the cluster, so you can load/unload modules within your slurm submission scripts or during interactive jobs. This is what they are intended for. List All Available Modules module av which will return an output that looks something like this: ------------------------------------------------- /modules/modulefiles ------------------------------------------------- R/3.6.2 cuda/10.1.243 (D) gcc/9.2.0 julia/1.1.1 openmpi/4.0.4 cmake/3.7.2 cuda/11.0.1 glxgears/1.0 jupyter/3.6.8 python/2.7.16 cmake/3.15.0 (D) fd3dspher/1.0 gmsh/4.4.1 mathematica/12.0 python/3.7.4 (D) cuda/8.0.61 gcc/5.5.0 gpu-burn/default mesa/19.0.8 qt/5.13.1 cuda/9.0.176 gcc/6.5.0 gromacs/2020.2C miniconda/3.7 stress/1.0.4 cuda/9.2.148 gcc/7.4.0 (D) gromacs/2020.2G (D) opencl/2.2.11 vtk/8.2.0 Note The output above is subject to change. Searching for Modules module av gcc That will only show you the available gcc modules. Loading Modules module load gcc/9.2.0 Note The convention is <app name>/<app version> . After loading a module, you can test is out by running whereis gcc , which will show you which gcc will be executed. Unloading Modules module unload gcc Unloading All Modules module purge List Currently Loaded Modules module list","title":"Using Modules"},{"location":"modules/using.html#using-modulefiles","text":"Using modulefiles is very simple. Keep in mind that the below commands will work anywhere on the cluster, so you can load/unload modules within your slurm submission scripts or during interactive jobs. This is what they are intended for.","title":"Using Modulefiles"},{"location":"modules/using.html#list-all-available-modules","text":"module av which will return an output that looks something like this: ------------------------------------------------- /modules/modulefiles ------------------------------------------------- R/3.6.2 cuda/10.1.243 (D) gcc/9.2.0 julia/1.1.1 openmpi/4.0.4 cmake/3.7.2 cuda/11.0.1 glxgears/1.0 jupyter/3.6.8 python/2.7.16 cmake/3.15.0 (D) fd3dspher/1.0 gmsh/4.4.1 mathematica/12.0 python/3.7.4 (D) cuda/8.0.61 gcc/5.5.0 gpu-burn/default mesa/19.0.8 qt/5.13.1 cuda/9.0.176 gcc/6.5.0 gromacs/2020.2C miniconda/3.7 stress/1.0.4 cuda/9.2.148 gcc/7.4.0 (D) gromacs/2020.2G (D) opencl/2.2.11 vtk/8.2.0 Note The output above is subject to change.","title":"List All Available Modules"},{"location":"modules/using.html#searching-for-modules","text":"module av gcc That will only show you the available gcc modules.","title":"Searching for Modules"},{"location":"modules/using.html#loading-modules","text":"module load gcc/9.2.0 Note The convention is <app name>/<app version> . After loading a module, you can test is out by running whereis gcc , which will show you which gcc will be executed.","title":"Loading Modules"},{"location":"modules/using.html#unloading-modules","text":"module unload gcc","title":"Unloading Modules"},{"location":"modules/using.html#unloading-all-modules","text":"module purge","title":"Unloading All Modules"},{"location":"modules/using.html#list-currently-loaded-modules","text":"module list","title":"List Currently Loaded Modules"},{"location":"slurm/index.html","text":"Introduction to Slurm: The Job Scheduler Slurm is the job scheduler we use in our cluster. More info about what a job scheduler is can be found in the introduction . Here we will go more into depth about some elements of the scheduler. There are many more features of Slurm that go beyond the scope of this guide, but all that you as a user needs to know should be available. Note Doing work on the login nodes can cause Unity to become sluggish for the entire user base. We have disincentivized this by setting limits on cpu and memory. Learn how to use srun interactive sessions to switch from a login node to a compute node. Partitions / Queues Our cluster has a number of slurm partitions defined, also known as a queue . As you may have guessed, you as the user request to use a specific partition based on what resources your job needs. To view all available partitions in Slurm, you can run the command sinfo . Partition Relative Wait Time Maximum Runtime cpu medium 1 day gpu medium 1 day cpu-long long 14 days gpu-long long 14 days cpu-preempt short 14 days (see below) gpu-preempt short 14 days (see below) Non-Standard Partitions Access to non-standard partitions (not listed above) is typically limited to those who purchased the hardware. Preempt Jobs can be killed and re-queued after two hours in the -preempt partition. If you don't want your job re-queued (but still killed), you can specify --no-requeue in your job. Jobs A job is an operation which the user submits to the cluster to run under allocated resources. There are two commands for this, srun and sbatch . srun is tied to your current session, and can allow you to interact with your job. sbatch is not tied to your current session, so you can start it and walk away. If you want to interact with your job and be able to walk away, you can use tmux to make a detachable session. (see below) SRUN An srun job is tied to your ssh session. If you break (ctrl+C) or close your ssh session during an srun job, the job will be killed . You can also make an interactive job, which will allow your job to take input from your keyboard. You can run bash in an interactive job to resume your work on a compute node just as you would on a login node. This is highly recommended. See SRUN Jobs for more information. SBATCH An sbatch job is submitted to the cluster with no information returned to the user other than a Job ID. An sbatch job will try to create a file in your current working directory that contains the results of your job. See SBATCH Jobs for more information. TMUX SRUN tmux # tmux session opens srun --pty -c 1 bash # interactive job on compute node opens with one cpu core sleep 3600; echo \"done\" # interactive job will have blinking cursor for an hour # > ctrl+b # tmux keyboard-shortcut command mode opens # > d # tmux session detaches, back to login node # at this point you can log off and log back in without killing the job tmux ls # print list of tmux sessions # first number on the left (call it X) is needed to re-attach the session tmux attach-session -t X # back to interactive job","title":"Introduction"},{"location":"slurm/index.html#introduction-to-slurm-the-job-scheduler","text":"Slurm is the job scheduler we use in our cluster. More info about what a job scheduler is can be found in the introduction . Here we will go more into depth about some elements of the scheduler. There are many more features of Slurm that go beyond the scope of this guide, but all that you as a user needs to know should be available. Note Doing work on the login nodes can cause Unity to become sluggish for the entire user base. We have disincentivized this by setting limits on cpu and memory. Learn how to use srun interactive sessions to switch from a login node to a compute node.","title":"Introduction to Slurm: The Job Scheduler"},{"location":"slurm/index.html#partitions-queues","text":"Our cluster has a number of slurm partitions defined, also known as a queue . As you may have guessed, you as the user request to use a specific partition based on what resources your job needs. To view all available partitions in Slurm, you can run the command sinfo . Partition Relative Wait Time Maximum Runtime cpu medium 1 day gpu medium 1 day cpu-long long 14 days gpu-long long 14 days cpu-preempt short 14 days (see below) gpu-preempt short 14 days (see below)","title":"Partitions / Queues"},{"location":"slurm/index.html#non-standard-partitions","text":"Access to non-standard partitions (not listed above) is typically limited to those who purchased the hardware.","title":"Non-Standard Partitions"},{"location":"slurm/index.html#preempt","text":"Jobs can be killed and re-queued after two hours in the -preempt partition. If you don't want your job re-queued (but still killed), you can specify --no-requeue in your job.","title":"Preempt"},{"location":"slurm/index.html#jobs","text":"A job is an operation which the user submits to the cluster to run under allocated resources. There are two commands for this, srun and sbatch . srun is tied to your current session, and can allow you to interact with your job. sbatch is not tied to your current session, so you can start it and walk away. If you want to interact with your job and be able to walk away, you can use tmux to make a detachable session. (see below)","title":"Jobs"},{"location":"slurm/index.html#srun","text":"An srun job is tied to your ssh session. If you break (ctrl+C) or close your ssh session during an srun job, the job will be killed . You can also make an interactive job, which will allow your job to take input from your keyboard. You can run bash in an interactive job to resume your work on a compute node just as you would on a login node. This is highly recommended. See SRUN Jobs for more information.","title":"SRUN"},{"location":"slurm/index.html#sbatch","text":"An sbatch job is submitted to the cluster with no information returned to the user other than a Job ID. An sbatch job will try to create a file in your current working directory that contains the results of your job. See SBATCH Jobs for more information.","title":"SBATCH"},{"location":"slurm/index.html#tmux-srun","text":"tmux # tmux session opens srun --pty -c 1 bash # interactive job on compute node opens with one cpu core sleep 3600; echo \"done\" # interactive job will have blinking cursor for an hour # > ctrl+b # tmux keyboard-shortcut command mode opens # > d # tmux session detaches, back to login node # at this point you can log off and log back in without killing the job tmux ls # print list of tmux sessions # first number on the left (call it X) is needed to re-attach the session tmux attach-session -t X # back to interactive job","title":"TMUX SRUN"},{"location":"slurm/sbatch.html","text":"Using SBATCH to Submit Jobs SBATCH is a non-blocking command, meaning there is not a circumstance where running the command will cause it to hold. Even if the resources requested are not available, the job will be thrown into the queue and will start to run once resources become available. The status of a job can be seen using squeue . squeue --me squeue -j YOUR_JOBID SBATCH is based around running a single file. That being said, you shouldn't need to specify any parameters in the command other than sbatch <batch file> , because you can specify all parameters in the command inside the file itself. The following is an example of a batch script. Please note that the top of the script must start with #!/bin/bash (or whatever interpreter you need, if you don't know, use bash), and then immediately follow with #SBATCH <param> parameters. An example of common SBATCH parameters and a simple script is below, this script will allocate 4 CPUs and one GPU in the GPU partition. #!/bin/bash #SBATCH -c 4 # Number of Cores per Task #SBATCH --mem=8192 # Requested Memory #SBATCH -p gpu # Partition #SBATCH -G 1 # Number of GPUs #SBATCH -t 01:00:00 # Job time limit #SBATCH -o slurm-%j.out # %j = job ID module load cuda/10 /modules/apps/cuda/10.1.243/samples/bin/x86_64/linux/release/deviceQuery This script should query the available GPUs, and print only one device to the specified file. Feel free to remove/modify any of the parameters in the script to suit your needs.","title":"SBATCH Jobs"},{"location":"slurm/sbatch.html#using-sbatch-to-submit-jobs","text":"SBATCH is a non-blocking command, meaning there is not a circumstance where running the command will cause it to hold. Even if the resources requested are not available, the job will be thrown into the queue and will start to run once resources become available. The status of a job can be seen using squeue . squeue --me squeue -j YOUR_JOBID SBATCH is based around running a single file. That being said, you shouldn't need to specify any parameters in the command other than sbatch <batch file> , because you can specify all parameters in the command inside the file itself. The following is an example of a batch script. Please note that the top of the script must start with #!/bin/bash (or whatever interpreter you need, if you don't know, use bash), and then immediately follow with #SBATCH <param> parameters. An example of common SBATCH parameters and a simple script is below, this script will allocate 4 CPUs and one GPU in the GPU partition. #!/bin/bash #SBATCH -c 4 # Number of Cores per Task #SBATCH --mem=8192 # Requested Memory #SBATCH -p gpu # Partition #SBATCH -G 1 # Number of GPUs #SBATCH -t 01:00:00 # Job time limit #SBATCH -o slurm-%j.out # %j = job ID module load cuda/10 /modules/apps/cuda/10.1.243/samples/bin/x86_64/linux/release/deviceQuery This script should query the available GPUs, and print only one device to the specified file. Feel free to remove/modify any of the parameters in the script to suit your needs.","title":"Using SBATCH to Submit Jobs"},{"location":"slurm/srun.html","text":"Using SRUN to Submit Jobs Note Usually, if you have to run a single application multiple times, or if you are trying to run a non-interactive application, you should use sbatch instead of srun, since sbatch allows you to specify parameters in the file, and is non-blocking (see below). SRUN is a so-called blocking command, as in it will not let you execute other commands until this command is finished (not necessarily the job, just the allocation). For example, if you run srun /bin/hostname and resources are available right away, the job will be sent out and the result saved into a file. If resources are not available, you will be stuck in the command while you are pending in the queue. Please note that like sbatch, you can run a batch file using srun. The command syntax is srun <options> [executable] <args> Options is where you can specify the resources you want for the executable, or define. The following are some of the options available; to see all available parameters run man srun . -c <num> Number of CPUs (threads) to allocate to the job per task -n <num> The number of tasks to allocate (for MPI) -G <num> Number of GPUs to allocate to the job --mem <num>[K|M|G|T] Memory to allocate to the job (in MB by default) -p <partition> Partition to submit the job to To run an interacitve job (in this case a bash prompt), the command might look like this ( --pty is the important option): srun -c 6 -p cpu --pty bash To run an application on the cluster that uses a GUI, you must use an interactive job, in addition to the --x11 argument: srun -c 6 -p cpu --pty --x11 xclock Note You cannot run an interactive/gui job using the sbatch command, you must use srun .","title":"SRUN Jobs"},{"location":"slurm/srun.html#using-srun-to-submit-jobs","text":"Note Usually, if you have to run a single application multiple times, or if you are trying to run a non-interactive application, you should use sbatch instead of srun, since sbatch allows you to specify parameters in the file, and is non-blocking (see below). SRUN is a so-called blocking command, as in it will not let you execute other commands until this command is finished (not necessarily the job, just the allocation). For example, if you run srun /bin/hostname and resources are available right away, the job will be sent out and the result saved into a file. If resources are not available, you will be stuck in the command while you are pending in the queue. Please note that like sbatch, you can run a batch file using srun. The command syntax is srun <options> [executable] <args> Options is where you can specify the resources you want for the executable, or define. The following are some of the options available; to see all available parameters run man srun . -c <num> Number of CPUs (threads) to allocate to the job per task -n <num> The number of tasks to allocate (for MPI) -G <num> Number of GPUs to allocate to the job --mem <num>[K|M|G|T] Memory to allocate to the job (in MB by default) -p <partition> Partition to submit the job to To run an interacitve job (in this case a bash prompt), the command might look like this ( --pty is the important option): srun -c 6 -p cpu --pty bash To run an application on the cluster that uses a GUI, you must use an interactive job, in addition to the --x11 argument: srun -c 6 -p cpu --pty --x11 xclock Note You cannot run an interactive/gui job using the sbatch command, you must use srun .","title":"Using SRUN to Submit Jobs"},{"location":"technical/nodelist.html","text":"Node List The Unity cluster is a heterogeneous cluster. We plan to keep it a heterogeneous cluster. To clear confusion, a node list is below, with slurm constraints to refine your node selection. Note You can use the -C flag in your slurm jobs to batch your jobs to a specific set of nodes. If you don't include a constraint, your job may land in any number of nodes in the partition. CPU Nodes Name Model CPU RAM Partitions Constraints cpu[001-008] Lenovo ThinkSystem SD530 2x Intel Xeon Gold 6126 (12 Cores, 24 Threads) 192 GiB (Node(s) 1,5-8), 384 GiB (Nodes 2-4) cpu cpu-long len-sd530_2018 avx avx2 avx512 intel linux-ubuntu20.04-skylake_avx512 cpu[013-025] Dell Poweredge R640 2x Intel Xeon Gold 6148 (20 Cores, 40 Threads) 192 GiB cpu cpu-long dell-r640_2020 avx avx2 avx512 intel linux-ubuntu20.04-skylake_avx512 ceewater-cpu[001-007] Lenovo ThinkSystem SR635 1x AMD EPYC-Rome 7402 (24 Cores, 48 Threads) 128 GiB ceewater_cjgleason-cpu ceewater_casey-cpu ceewater_kandread-cpu cpu-preempt ceewater_len-sr635_2020 avx avx2 amd linux-ubuntu20.04-zen2 astroth-cpu[001-008] SuperMicro SBI-4429P 2x Xeon Silver 4215R (8 Cores, 16 Threads) 192 GiB astroth-cpu cpu-preempt astroth_smicro-sbi4429p_2021 avx avx2 avx512 intel linux-ubuntu20.04-cascadelake zhoulin-cpu[001-006] Lenovo SR645 2x AMD EPYC 7702 (128 Cores, 256 Threads) 512 GiB zhoulin-cpu cpu-preempt zhoulin_len-sr645_2021 avx avx2 amd linux-ubuntu20.04-zen2 toltec-cpu[001-006] Dell R640 2x Intel Xeon Gold 5218 CPU (32 Cores, 64 Threads) 383 GiB toltec-cpu cpu-preempt toltec_dell-r640_2021 avx avx2 avx512 intel linux-ubuntu20.04-cascadelake gaoseismolab-cpu[001-005] Lenovo SR630 v2 2x Intel Xeon Platinum 8358 (32 Cores, 64 Threads) 512 GiB gaoseismolab-cpu cpu-preempt avx avx2 avx512 intel linux-ubuntu20.04-icelake uri-cpu[001-005] Intel S2600BPB 2x Intel Xeon Gold 6238R (28 Cores, 56 Threads) 512 GiB uri-cpu cpu cpu-long avx avx2 avx512 intel linux-ubuntu20.04-cascadelake GPU Nodes Name Model CPU GPU RAM Partitions Constraints gpu[001-002] Lenovo ThinkSystem SR650 2x Intel Xeon Silver 4110 (8 Cores, 16 Threads) 2x NVIDIA Tesla V100 (16GB VRAM) 192 GiB gpu gpu-long len-sr650_2018 avx avx2 avx512 v100 intel linux-ubuntu20.04-skylake_avx512 gpu[003-004] Dell Poweredge R740 2x Intel Xeon Gold 6140 (18 Cores, 36 Threads) 2x NVIDIA Tesla V100 (16GB VRAM) 192 GiB gpu gpu-long len-sr650_2018 avx avx2 avx512 v100 intel linux-ubuntu20.04-skylake_avx512 ials-gpu[001-033] Atipa 2x Intel Xeon Silver 4214R (12 Cores, 12 Threads) 8x NVIDIA RTX 2080ti (12GB VRAM) 192 GiB ials-gpu gpu gpu-long ials_gigabyte_2020 avx avx2 avx512 2080ti intel linux-ubuntu20.04-cascadelake astroth-gpu[001-003] ASRock AMD Ryzen Threadripper 1900X (8 Cores, 16 Threads) 2x NVIDIA RTX 2080 (8GB VRAM) 32 GiB astroth-gpu astro_asrock_x399_2020 avx avx2 2080 amd linux-ubuntu20.04-zen ece-gpu[001-002] Lenovo SR670 2x Intel Xeon Gold 6226R CPU (64 Cores, 128 Threads) 4x NVIDIA Tesla A100 384 GiB ece-gpu ece_len-sr670_2021 avx avx2 avx512 a100 intel gypsum-gpu[001-025] ASUSTeK ESC4000 G3 Series 2x Intel Xeon E5-2620 v3 (12 Cores, 24 Threads) 4x NVIDIA Tesla M40 256 GiB gypsum-m40-phd gypsum-m40-ms gypsum-m40-course gpu-preempt linux-ubuntu20.04-haswell gypsum-gpu[026-099] ASUSTeK ESC4000 G3 Series 2x Intel Xeon E5-2620 v3 (12 Cores, 24 Threads) 4x NVIDIA GeForce GTX TITAN X gypsum-titanx-phd gypsum-titanx-ms gypsum-titanx-course gpu-preempt linux-ubuntu20.04-haswell gypsum-gpu[104-156] TYAN B7109F77DV14HR-2T-N 2x Intel Xeon Silver 4116 (24 Cores, 48 Threads) 8x NVIDIA GeForce GTX 1080 Ti 384 GiB gypsum-1080ti-phd gypsum-1080ti-ms gypsum-1080ti-course gpu-preempt linux-ubuntu20.04-skylake_avx512 gypsum-gpu[157-181] TYAN B7109F77DV14HR-2T-N 2x Intel Xeon Silver 4116 (24 Cores, 48 Threads) 8x NVIDIA GeForce GTX 2080 Ti 384 GiB gypsum-2080ti-phd gypsum-2080ti-ms gypsum-2080ti-course gpu-preempt gypsum-gpu[182-189] Supermicro SYS-4029GP-TRT2 2x Intel Xeon Silver 4116 (24 Cores, 48 Threads) 8x NVIDIA Quadro RTX 8000 384 GiB gypsum-rtx8000-phd gypsum-rtx8000-ms gypsum-rtx8000-course gpu-preempt gypsum-gpu[190-192] Supermicro SYS-4029GP-TRT2 2x Intel Xeon Silver 4116 (24 Cores, 48 Threads) 8x NVIDIA GeForce GTX 2080 Ti 384 GiB gypsum-2080ti-phd gypsum-2080ti-ms gypsum-2080ti-course gpu-preempt","title":"Node List"},{"location":"technical/nodelist.html#node-list","text":"The Unity cluster is a heterogeneous cluster. We plan to keep it a heterogeneous cluster. To clear confusion, a node list is below, with slurm constraints to refine your node selection. Note You can use the -C flag in your slurm jobs to batch your jobs to a specific set of nodes. If you don't include a constraint, your job may land in any number of nodes in the partition.","title":"Node List"},{"location":"technical/nodelist.html#cpu-nodes","text":"Name Model CPU RAM Partitions Constraints cpu[001-008] Lenovo ThinkSystem SD530 2x Intel Xeon Gold 6126 (12 Cores, 24 Threads) 192 GiB (Node(s) 1,5-8), 384 GiB (Nodes 2-4) cpu cpu-long len-sd530_2018 avx avx2 avx512 intel linux-ubuntu20.04-skylake_avx512 cpu[013-025] Dell Poweredge R640 2x Intel Xeon Gold 6148 (20 Cores, 40 Threads) 192 GiB cpu cpu-long dell-r640_2020 avx avx2 avx512 intel linux-ubuntu20.04-skylake_avx512 ceewater-cpu[001-007] Lenovo ThinkSystem SR635 1x AMD EPYC-Rome 7402 (24 Cores, 48 Threads) 128 GiB ceewater_cjgleason-cpu ceewater_casey-cpu ceewater_kandread-cpu cpu-preempt ceewater_len-sr635_2020 avx avx2 amd linux-ubuntu20.04-zen2 astroth-cpu[001-008] SuperMicro SBI-4429P 2x Xeon Silver 4215R (8 Cores, 16 Threads) 192 GiB astroth-cpu cpu-preempt astroth_smicro-sbi4429p_2021 avx avx2 avx512 intel linux-ubuntu20.04-cascadelake zhoulin-cpu[001-006] Lenovo SR645 2x AMD EPYC 7702 (128 Cores, 256 Threads) 512 GiB zhoulin-cpu cpu-preempt zhoulin_len-sr645_2021 avx avx2 amd linux-ubuntu20.04-zen2 toltec-cpu[001-006] Dell R640 2x Intel Xeon Gold 5218 CPU (32 Cores, 64 Threads) 383 GiB toltec-cpu cpu-preempt toltec_dell-r640_2021 avx avx2 avx512 intel linux-ubuntu20.04-cascadelake gaoseismolab-cpu[001-005] Lenovo SR630 v2 2x Intel Xeon Platinum 8358 (32 Cores, 64 Threads) 512 GiB gaoseismolab-cpu cpu-preempt avx avx2 avx512 intel linux-ubuntu20.04-icelake uri-cpu[001-005] Intel S2600BPB 2x Intel Xeon Gold 6238R (28 Cores, 56 Threads) 512 GiB uri-cpu cpu cpu-long avx avx2 avx512 intel linux-ubuntu20.04-cascadelake","title":"CPU Nodes"},{"location":"technical/nodelist.html#gpu-nodes","text":"Name Model CPU GPU RAM Partitions Constraints gpu[001-002] Lenovo ThinkSystem SR650 2x Intel Xeon Silver 4110 (8 Cores, 16 Threads) 2x NVIDIA Tesla V100 (16GB VRAM) 192 GiB gpu gpu-long len-sr650_2018 avx avx2 avx512 v100 intel linux-ubuntu20.04-skylake_avx512 gpu[003-004] Dell Poweredge R740 2x Intel Xeon Gold 6140 (18 Cores, 36 Threads) 2x NVIDIA Tesla V100 (16GB VRAM) 192 GiB gpu gpu-long len-sr650_2018 avx avx2 avx512 v100 intel linux-ubuntu20.04-skylake_avx512 ials-gpu[001-033] Atipa 2x Intel Xeon Silver 4214R (12 Cores, 12 Threads) 8x NVIDIA RTX 2080ti (12GB VRAM) 192 GiB ials-gpu gpu gpu-long ials_gigabyte_2020 avx avx2 avx512 2080ti intel linux-ubuntu20.04-cascadelake astroth-gpu[001-003] ASRock AMD Ryzen Threadripper 1900X (8 Cores, 16 Threads) 2x NVIDIA RTX 2080 (8GB VRAM) 32 GiB astroth-gpu astro_asrock_x399_2020 avx avx2 2080 amd linux-ubuntu20.04-zen ece-gpu[001-002] Lenovo SR670 2x Intel Xeon Gold 6226R CPU (64 Cores, 128 Threads) 4x NVIDIA Tesla A100 384 GiB ece-gpu ece_len-sr670_2021 avx avx2 avx512 a100 intel gypsum-gpu[001-025] ASUSTeK ESC4000 G3 Series 2x Intel Xeon E5-2620 v3 (12 Cores, 24 Threads) 4x NVIDIA Tesla M40 256 GiB gypsum-m40-phd gypsum-m40-ms gypsum-m40-course gpu-preempt linux-ubuntu20.04-haswell gypsum-gpu[026-099] ASUSTeK ESC4000 G3 Series 2x Intel Xeon E5-2620 v3 (12 Cores, 24 Threads) 4x NVIDIA GeForce GTX TITAN X gypsum-titanx-phd gypsum-titanx-ms gypsum-titanx-course gpu-preempt linux-ubuntu20.04-haswell gypsum-gpu[104-156] TYAN B7109F77DV14HR-2T-N 2x Intel Xeon Silver 4116 (24 Cores, 48 Threads) 8x NVIDIA GeForce GTX 1080 Ti 384 GiB gypsum-1080ti-phd gypsum-1080ti-ms gypsum-1080ti-course gpu-preempt linux-ubuntu20.04-skylake_avx512 gypsum-gpu[157-181] TYAN B7109F77DV14HR-2T-N 2x Intel Xeon Silver 4116 (24 Cores, 48 Threads) 8x NVIDIA GeForce GTX 2080 Ti 384 GiB gypsum-2080ti-phd gypsum-2080ti-ms gypsum-2080ti-course gpu-preempt gypsum-gpu[182-189] Supermicro SYS-4029GP-TRT2 2x Intel Xeon Silver 4116 (24 Cores, 48 Threads) 8x NVIDIA Quadro RTX 8000 384 GiB gypsum-rtx8000-phd gypsum-rtx8000-ms gypsum-rtx8000-course gpu-preempt gypsum-gpu[190-192] Supermicro SYS-4029GP-TRT2 2x Intel Xeon Silver 4116 (24 Cores, 48 Threads) 8x NVIDIA GeForce GTX 2080 Ti 384 GiB gypsum-2080ti-phd gypsum-2080ti-ms gypsum-2080ti-course gpu-preempt","title":"GPU Nodes"},{"location":"technical/partitionlist.html","text":"Partition List General Use Partitions General use partitions are open to use by all users of Unity. Name Time Limit Max CPUs Per Node Comments cpu 1 DAY 40 CPU short partition cpu-long 14 DAYS 40 CPU long partition cpu-preempt 14 DAYS 256 Priority jobs can preempt (requeue) your jobs after a 2 hour grace time. It is a good idea to stick to jobs with checkpoint support for this partition. gpu 1 DAY GPU short partition gpu-long 14 DAYS GPU long partition gpu-preempt 14 DAYS Priority jobs can preempt (requeue) your jobs after a 2 hour grace time. It is a good idea to stick to jobs with checkpoint support for this partition. Gypsum Cluster Partitions Gypsum users, depending on the type, have access to these partitions. Name Time Limit Comments gypsum-m40-phd 7 DAYS m40 GPU partition for pHd gypsum-m40-ms 7 DAYS m40 GPU partition for MS gypsum-m40-course 7 DAYS m40 GPU partition for Courses gypsum-titanx-phd 7 DAYS titanx GPU partition for pHd gypsum-titanx-ms 7 DAYS titanx GPU partition for MS gypsum-titanx-course 7 DAYS titanx GPU partition for Courses gypsum-1080ti-phd 7 DAYS 1080ti GPU partition for pHd gypsum-1080ti-ms 7 DAYS 1080ti GPU partition for MS gypsum-1080ti-course 7 DAYS 1080ti GPU partition for Courses gypsum-2080ti-phd 7 DAYS 2080ti GPU partition for pHd gypsum-2080ti-ms 7 DAYS 2080ti GPU partition for MS gypsum-2080ti-course 7 DAYS 2080ti GPU partition for Courses IALS Cluster Partitions If you are an authorized IALS member on Unity, you can use these partitions. Name Time Limit Comments ials-gpu 14 DAYS GPU partition for IALS Other Priority Partitions These are the remaining priority partitions for smaller installations purchased for specific labs by themselves. Name Time Limit Comments ceewater_cjgleason-cpu Unlimited ceewater CPU partition for cjgleason group ceewater_casey-cpu Unlimited ceewater CPU partition for casey group ceewater_kandread-cpu Unlimited ceewater CPU partition for kandread group astroth-cpu Unlimited astroth CPU partition zhoulin-cpu Unlimited zhoulin CPU partition toltec-cpu Unlimited toltec CPU partition gaoseismolab-cpu Unlimited gaoseismolab CPU partition uri-cpu Unlimited URI CPU partition ece-gpu 5 DAYS ece partition for ECE courses only","title":"Partition List"},{"location":"technical/partitionlist.html#partition-list","text":"","title":"Partition List"},{"location":"technical/partitionlist.html#general-use-partitions","text":"General use partitions are open to use by all users of Unity. Name Time Limit Max CPUs Per Node Comments cpu 1 DAY 40 CPU short partition cpu-long 14 DAYS 40 CPU long partition cpu-preempt 14 DAYS 256 Priority jobs can preempt (requeue) your jobs after a 2 hour grace time. It is a good idea to stick to jobs with checkpoint support for this partition. gpu 1 DAY GPU short partition gpu-long 14 DAYS GPU long partition gpu-preempt 14 DAYS Priority jobs can preempt (requeue) your jobs after a 2 hour grace time. It is a good idea to stick to jobs with checkpoint support for this partition.","title":"General Use Partitions"},{"location":"technical/partitionlist.html#gypsum-cluster-partitions","text":"Gypsum users, depending on the type, have access to these partitions. Name Time Limit Comments gypsum-m40-phd 7 DAYS m40 GPU partition for pHd gypsum-m40-ms 7 DAYS m40 GPU partition for MS gypsum-m40-course 7 DAYS m40 GPU partition for Courses gypsum-titanx-phd 7 DAYS titanx GPU partition for pHd gypsum-titanx-ms 7 DAYS titanx GPU partition for MS gypsum-titanx-course 7 DAYS titanx GPU partition for Courses gypsum-1080ti-phd 7 DAYS 1080ti GPU partition for pHd gypsum-1080ti-ms 7 DAYS 1080ti GPU partition for MS gypsum-1080ti-course 7 DAYS 1080ti GPU partition for Courses gypsum-2080ti-phd 7 DAYS 2080ti GPU partition for pHd gypsum-2080ti-ms 7 DAYS 2080ti GPU partition for MS gypsum-2080ti-course 7 DAYS 2080ti GPU partition for Courses","title":"Gypsum Cluster Partitions"},{"location":"technical/partitionlist.html#ials-cluster-partitions","text":"If you are an authorized IALS member on Unity, you can use these partitions. Name Time Limit Comments ials-gpu 14 DAYS GPU partition for IALS","title":"IALS Cluster Partitions"},{"location":"technical/partitionlist.html#other-priority-partitions","text":"These are the remaining priority partitions for smaller installations purchased for specific labs by themselves. Name Time Limit Comments ceewater_cjgleason-cpu Unlimited ceewater CPU partition for cjgleason group ceewater_casey-cpu Unlimited ceewater CPU partition for casey group ceewater_kandread-cpu Unlimited ceewater CPU partition for kandread group astroth-cpu Unlimited astroth CPU partition zhoulin-cpu Unlimited zhoulin CPU partition toltec-cpu Unlimited toltec CPU partition gaoseismolab-cpu Unlimited gaoseismolab CPU partition uri-cpu Unlimited URI CPU partition ece-gpu 5 DAYS ece partition for ECE courses only","title":"Other Priority Partitions"},{"location":"technical/storage.html","text":"Storage Below if a table of all available storage on Unity. Mountpoint Name Location Type Quota Description /home Home directories Everywhere HDD 10 GB Home directories should be used only for user init files. /work Work directories Everywhere SSD 3 TB Work should be used as the primary location for running cluster jobs from. There are 2 folders in here a normal user will be concerned with. /work/[username], which is your personal storage, and /work/[lab id], which is a shared folder that can be used by everyone in the lab to share files. Both have the same quota of 3TB for now. /project Project directories Everywhere HDD Varying Project directories are created on request. Good for large dataset storage or any larger storage that is not directly used for job I/O. Email hpc@umass.edu to request. A common use case is generating job output in /work and copying to permanent storage in /project afterwards. Not for job I/O /nese NESE mounts Everywhere HDD/Tape Varying Images available from the northeast storage exchange can be found here. Not for job I/O /nas Buy-in NAS mounts Everywhere Varying Varying This location is where the mounts for buy-in NAS hardware is located on Unity. For users who purchased storage nodes for their own use on Unity only. /scratch Scratch space Everywhere (Intended for Compute) SSD 40 TB / user, cleared at the end of job /scratch/[nodeid]/[jobid] is created when a job is started. That folder is assigned to $TMP and deleted after the job is complete. Please use this directory like you would /tmp on a normal system - for temporary job I/O that you do not need after the job is completed. Useful for jobs that require large amounts of intermediary files, which is not needed after job completion. /gypsum Gypsum devices Everywhere HDD Varying For users migrating from the Gypsum cluster to the Unity clusters, you will find all your old storage here. /old Old mounts Everywhere (Read-Only) Varying Varying Old filesystems which are deprecated live here until they are deleted.","title":"Storage"},{"location":"technical/storage.html#storage","text":"Below if a table of all available storage on Unity. Mountpoint Name Location Type Quota Description /home Home directories Everywhere HDD 10 GB Home directories should be used only for user init files. /work Work directories Everywhere SSD 3 TB Work should be used as the primary location for running cluster jobs from. There are 2 folders in here a normal user will be concerned with. /work/[username], which is your personal storage, and /work/[lab id], which is a shared folder that can be used by everyone in the lab to share files. Both have the same quota of 3TB for now. /project Project directories Everywhere HDD Varying Project directories are created on request. Good for large dataset storage or any larger storage that is not directly used for job I/O. Email hpc@umass.edu to request. A common use case is generating job output in /work and copying to permanent storage in /project afterwards. Not for job I/O /nese NESE mounts Everywhere HDD/Tape Varying Images available from the northeast storage exchange can be found here. Not for job I/O /nas Buy-in NAS mounts Everywhere Varying Varying This location is where the mounts for buy-in NAS hardware is located on Unity. For users who purchased storage nodes for their own use on Unity only. /scratch Scratch space Everywhere (Intended for Compute) SSD 40 TB / user, cleared at the end of job /scratch/[nodeid]/[jobid] is created when a job is started. That folder is assigned to $TMP and deleted after the job is complete. Please use this directory like you would /tmp on a normal system - for temporary job I/O that you do not need after the job is completed. Useful for jobs that require large amounts of intermediary files, which is not needed after job completion. /gypsum Gypsum devices Everywhere HDD Varying For users migrating from the Gypsum cluster to the Unity clusters, you will find all your old storage here. /old Old mounts Everywhere (Read-Only) Varying Varying Old filesystems which are deprecated live here until they are deleted.","title":"Storage"},{"location":"uploading-files/index.html","text":"Uploading Files to the Unity Filesystem The only way to add files to the Unity filesystem is through an SSL encrypted connection. It can be done with FileZilla (recommended), in JupyterLab, or in the command line. Note Uploading files using American residential internet is typically very slow. UMass Amherst has a fibre line going directly to MGHPCC to improve speeds. Your Key File When you set up your Unity account, you chose between PuTTY ( .ppk ) and OpenSSH. ( .rsa ) scp and rsync use OpenSSH, and FileZilla prefers .ppk but can work with .rsa . Depending on which software you use, you can generate one of each. You can also convert between these keys using a program like PuttyGEN. Configuring SSH Keys Account Settings FileZilla FileZilla can use either an .rsa or a .ppk private key, but the 'Browse' button will show only .ppk files. To use an .rsa key, type in the path to the keyfile by hand. This guide assumes that your key lives at ~/.ssh/KEYFILE , but you can substitute this path. You can install FileZilla here FileZilla may ask you if you want to install McAfee, you probably don't. If you don't have antivirus already, you probably should. The FileZilla installer executable can be sometimes marked as a virus, it isn't. Select the Site Manager in FileZilla: Create a New Site: Fill in the Fields: Type a name for the site under My Sites on the left Protocol: SFTP Host: unity.rc.umass.edu User: your email but replace the . and @ with _ Key File: /path/to/your/keyfile This configuration is saved automatically. You can use the 'Connect' button in the bottom right to open an explorer on the Unity Filesystem,and you can drag and drop your files across the two panels. Properly connected, FileZilla should look like this: JupyterLab Start a session in JupyterLab. Navigate to your home directory, and click the upload button. CLI It's best to try this after you have already successfully connected to Unity with OpenSSH. As these are CLI procedures, the first thing you need to do is open your terminal and navigate to the directory (folder) where the files you want to upload are located. Alternatively you can use absolute paths in your command and skip this step. # Windows cd C:/Users/YOUR_NAME/Desktop # Linux cd /home/$USER/Desktop # Mac cd /Users/YOUR_NAME/Desktop Assuming, of course, that the files you want to upload are located in your desktop directory. And in the Windows case, assuming that the drive you want to copy from is the C drive. Note If your file name contains spaces, you will have to put it in quotes. SCP OpenSSH comes with the scp command, which uses the same argument structure as cp (copy) but with the added benefit of referencing the OpenSSH config file ( ~/.ssh/config ). This is how I can use unity as part of a command, because the OpenSSH config file contains the connection information for host unity . # single file scp FILE_NAME unity:~ # entire directory scp -r DIRECTORY_NAME unity:~ This will copy the files in question to your Unity home directory. You could also upload to elsewhere on the Unity filesystem, wherever you have permissions. Note -r in many commands is short for 'recursive'. It means to recursively open directories to ensure that all contained files are accounted for. Note ~ in the terminal represents your home directory. This is C:/Users/YOUR_NAME in Windows, /home/YOUR_NAME in Linux, and /Users/YOUR_NAME in Mac . RSYNC rsync can be installed on Linux and Mac. The syntax is the same as scp . It also references the OpenSSH config file. It's recommended to use the -tlp flags so that timestamps, relative links, and permissions are preserved, respectively. # single file rsync -tlp FILE_NAME unity:~ # entire directory rsync -rtlp DIRECTORY_NAME unity:~ Globus Coming soon!","title":"Uploading Files"},{"location":"uploading-files/index.html#uploading-files-to-the-unity-filesystem","text":"The only way to add files to the Unity filesystem is through an SSL encrypted connection. It can be done with FileZilla (recommended), in JupyterLab, or in the command line. Note Uploading files using American residential internet is typically very slow. UMass Amherst has a fibre line going directly to MGHPCC to improve speeds.","title":"Uploading Files to the Unity Filesystem"},{"location":"uploading-files/index.html#your-key-file","text":"When you set up your Unity account, you chose between PuTTY ( .ppk ) and OpenSSH. ( .rsa ) scp and rsync use OpenSSH, and FileZilla prefers .ppk but can work with .rsa . Depending on which software you use, you can generate one of each. You can also convert between these keys using a program like PuttyGEN. Configuring SSH Keys Account Settings","title":"Your Key File"},{"location":"uploading-files/index.html#filezilla","text":"FileZilla can use either an .rsa or a .ppk private key, but the 'Browse' button will show only .ppk files. To use an .rsa key, type in the path to the keyfile by hand. This guide assumes that your key lives at ~/.ssh/KEYFILE , but you can substitute this path. You can install FileZilla here FileZilla may ask you if you want to install McAfee, you probably don't. If you don't have antivirus already, you probably should. The FileZilla installer executable can be sometimes marked as a virus, it isn't. Select the Site Manager in FileZilla: Create a New Site: Fill in the Fields: Type a name for the site under My Sites on the left Protocol: SFTP Host: unity.rc.umass.edu User: your email but replace the . and @ with _ Key File: /path/to/your/keyfile This configuration is saved automatically. You can use the 'Connect' button in the bottom right to open an explorer on the Unity Filesystem,and you can drag and drop your files across the two panels. Properly connected, FileZilla should look like this:","title":"FileZilla"},{"location":"uploading-files/index.html#jupyterlab","text":"Start a session in JupyterLab. Navigate to your home directory, and click the upload button.","title":"JupyterLab"},{"location":"uploading-files/index.html#cli","text":"It's best to try this after you have already successfully connected to Unity with OpenSSH. As these are CLI procedures, the first thing you need to do is open your terminal and navigate to the directory (folder) where the files you want to upload are located. Alternatively you can use absolute paths in your command and skip this step. # Windows cd C:/Users/YOUR_NAME/Desktop # Linux cd /home/$USER/Desktop # Mac cd /Users/YOUR_NAME/Desktop Assuming, of course, that the files you want to upload are located in your desktop directory. And in the Windows case, assuming that the drive you want to copy from is the C drive. Note If your file name contains spaces, you will have to put it in quotes.","title":"CLI"},{"location":"uploading-files/index.html#scp","text":"OpenSSH comes with the scp command, which uses the same argument structure as cp (copy) but with the added benefit of referencing the OpenSSH config file ( ~/.ssh/config ). This is how I can use unity as part of a command, because the OpenSSH config file contains the connection information for host unity . # single file scp FILE_NAME unity:~ # entire directory scp -r DIRECTORY_NAME unity:~ This will copy the files in question to your Unity home directory. You could also upload to elsewhere on the Unity filesystem, wherever you have permissions. Note -r in many commands is short for 'recursive'. It means to recursively open directories to ensure that all contained files are accounted for. Note ~ in the terminal represents your home directory. This is C:/Users/YOUR_NAME in Windows, /home/YOUR_NAME in Linux, and /Users/YOUR_NAME in Mac .","title":"SCP"},{"location":"uploading-files/index.html#rsync","text":"rsync can be installed on Linux and Mac. The syntax is the same as scp . It also references the OpenSSH config file. It's recommended to use the -tlp flags so that timestamps, relative links, and permissions are preserved, respectively. # single file rsync -tlp FILE_NAME unity:~ # entire directory rsync -rtlp DIRECTORY_NAME unity:~","title":"RSYNC"},{"location":"uploading-files/index.html#globus","text":"Coming soon!","title":"Globus"}]}